import torch
from torch.utils.data import Dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
import pandas as pd
import numpy as np
import os

# ---------------- 1. Configuration des Chemins ----------------
# Le dossier o√π se trouve le mod√®le vide (t√©l√©charg√©)
MODEL_PATH = "../ml_models/mbert"
# Le dossier o√π on sauvegarde le mod√®le entra√Æn√© (on √©crase le pr√©c√©dent)
SAVE_PATH = "../ml_models/mbert"

print(f"Chargement du mod√®le depuis : {os.path.abspath(MODEL_PATH)}")

# ---------------- 2. Pr√©paration des Datasets ----------------
# Dataset d'entra√Ænement
df_train = pd.read_csv("../data/dataset_train.csv")
# On renomme pour que ce soit clair, et on s'assure d'avoir les bonnes colonnes
df_train = df_train.rename(columns={"review_text": "text"}) 
# Si ta colonne s'appelle 'sentiment_label' (str) ou 'sentiment_id' (int), adapte ici :
if "sentiment_id" in df_train.columns:
    df_train = df_train.rename(columns={"sentiment_id": "label"})

# Dataset de test
df_test = pd.read_csv("../data/dataset_test.csv")
df_test = df_test.rename(columns={"review_text": "text"})
if "sentiment_id" in df_test.columns:
    df_test = df_test.rename(columns={"sentiment_id": "label"})

# Classe Dataset (Celle que tu avais √©tait tr√®s bien !)
class SentimentDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_len=128):
        self.data = dataframe
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        row = self.data.iloc[idx]
        text = str(row["text"])
        
        # Gestion s√©curis√©e du label
        label = int(row["label"])
            
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding="max_length",
            max_length=self.max_len,
            return_tensors="pt"
        )
        item = {key: val.squeeze(0) for key, val in encoding.items()}
        item["labels"] = torch.tensor(label, dtype=torch.long)
        return item

# ---------------- 3. Chargement Tokenizer & Mod√®le ----------------
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
print("Tokenizer charg√© !")

# Cr√©ation des objets Dataset (C'est ICI que tu avais une erreur)
train_dataset = SentimentDataset(df_train, tokenizer)
eval_dataset = SentimentDataset(df_test, tokenizer)

# Chargement du mod√®le
model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_PATH,
    num_labels=3,
    ignore_mismatched_sizes=True 
    # id2label et label2id sont d√©j√† dans le config.json gr√¢ce au script de download
)

# ---------------- 4. Entra√Ænement ----------------
training_args = TrainingArguments(
    output_dir="./results",          # Dossier temporaire pour les checkpoints
    num_train_epochs=3,              # 3 passages complets
    per_device_train_batch_size=8,   # R√©duit √† 8 pour √©viter de saturer la m√©moire
    per_device_eval_batch_size=16,
    weight_decay=0.01,
    logging_steps=10,
    evaluation_strategy="epoch",     # Evaluer √† la fin de chaque √©poque
    save_strategy="no",              # On sauvegarde manuellement √† la fin
    learning_rate=2e-5,              # Vitesse d'apprentissage standard pour BERT
)

def compute_metrics(p):
    preds = np.argmax(p.predictions, axis=1)
    return {"accuracy": (preds == p.label_ids).mean()}

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset, # <--- CORRECTION : on passe l'objet Dataset, pas le DataFrame
    eval_dataset=eval_dataset,   # <--- CORRECTION : idem
    compute_metrics=compute_metrics
)

print("üöÄ D√©marrage de l'entra√Ænement...")
trainer.train()

# ---------------- 5. Sauvegarde ----------------
print(f"üíæ Sauvegarde du mod√®le entra√Æn√© dans : {os.path.abspath(SAVE_PATH)}")
model.save_pretrained(SAVE_PATH)
tokenizer.save_pretrained(SAVE_PATH)
print("‚úÖ Termin√© ! Le mod√®le est pr√™t pour l'API.")